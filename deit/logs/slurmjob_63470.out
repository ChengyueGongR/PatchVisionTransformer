Sun Jan  3 10:03:09 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  TITAN V             On   | 00000000:04:00.0 Off |                  N/A |
| 28%   34C    P2    37W / 250W |      0MiB / 12066MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  TITAN V             On   | 00000000:05:00.0 Off |                  N/A |
| 28%   38C    P8    25W / 250W |      0MiB / 12066MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  TITAN V             On   | 00000000:08:00.0 Off |                  N/A |
| 30%   41C    P8    26W / 250W |      0MiB / 12066MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  TITAN V             On   | 00000000:09:00.0 Off |                  N/A |
| 29%   40C    P8    25W / 250W |      0MiB / 12066MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   4  TITAN V             On   | 00000000:84:00.0 Off |                  N/A |
| 28%   35C    P8    26W / 250W |      0MiB / 12066MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   5  TITAN V             On   | 00000000:85:00.0 Off |                  N/A |
| 28%   33C    P8    24W / 250W |      0MiB / 12066MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   6  TITAN V             On   | 00000000:88:00.0 Off |                  N/A |
| 28%   31C    P8    23W / 250W |      0MiB / 12066MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   7  TITAN V             On   | 00000000:89:00.0 Off |                  N/A |
| 28%   34C    P8    25W / 250W |      0MiB / 12066MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
top - 10:03:10 up 57 days, 20:06,  0 users,  load average: 6.17, 8.80, 17.03
Tasks: 477 total,   2 running, 261 sleeping,   0 stopped,   0 zombie
%Cpu(s): 27.2 us,  4.6 sy,  0.0 ni, 68.1 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem : 26403120+total, 65802344 free,  2568560 used, 19566028+buff/cache
KiB Swap:  3905532 total,  3881980 free,    23552 used. 25960422+avail Mem 

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 1832 root      20   0       0      0      0 S  16.7  0.0   8855:15 [nv_queue]
 1540 root      20   0       0      0      0 R  11.1  0.0   6533:11 [nv_queue]
 1605 root      20   0       0      0      0 S  11.1  0.0   8252:04 [nv_queue]
 1781 root      20   0       0      0      0 S  11.1  0.0   9063:00 [nv_queue]
 1807 root      20   0       0      0      0 S  11.1  0.0   9270:50 [nv_queue]
 1816 root      20   0       0      0      0 S  11.1  0.0   7420:57 [nv_queue]
25518 cygong    20   0   32844   3836   3052 R  11.1  0.0   0:00.04 top -bn 1 +
 1714 root      20   0       0      0      0 S   5.6  0.0   7375:12 [nv_queue]
Processing /u/cygong/Desktop/timm_mlp
Requirement already satisfied: torch>=1.0 in /u/cygong/anaconda/envs/pytorch/lib/python3.8/site-packages (from timm==0.3.2) (1.7.1)
Requirement already satisfied: torchvision in /u/cygong/anaconda/envs/pytorch/lib/python3.8/site-packages (from timm==0.3.2) (0.8.2)
Requirement already satisfied: typing_extensions in /u/cygong/anaconda/envs/pytorch/lib/python3.8/site-packages (from torch>=1.0->timm==0.3.2) (3.7.4.3)
Requirement already satisfied: numpy in /u/cygong/anaconda/envs/pytorch/lib/python3.8/site-packages (from torch>=1.0->timm==0.3.2) (1.19.2)
Requirement already satisfied: pillow>=4.1.1 in /u/cygong/anaconda/envs/pytorch/lib/python3.8/site-packages (from torchvision->timm==0.3.2) (8.0.1)
Building wheels for collected packages: timm
  Building wheel for timm (setup.py): started
  Building wheel for timm (setup.py): finished with status 'done'
  Created wheel for timm: filename=timm-0.3.2-py3-none-any.whl size=240952 sha256=824727cfb271431e34d7f697d5756b5f04f0a7890a650a116371ecc238492acf
  Stored in directory: /tmp/pip-ephem-wheel-cache-syb5fttc/wheels/7c/92/4d/f70b9c15c01aed89852b0201dd28cb5d80a328aa9b8459f535
Successfully built timm
Installing collected packages: timm
  Attempting uninstall: timm
    Found existing installation: timm 0.3.2
    Uninstalling timm-0.3.2:
      Successfully uninstalled timm-0.3.2
Successfully installed timm-0.3.2
| distributed init (rank 3): env://
| distributed init (rank 0): env://
| distributed init (rank 1): env://
| distributed init (rank 2): env://
Namespace(aa='rand-m9-mstd0.5-inc1', batch_size=256, clip_grad=None, color_jitter=0.4, cooldown_epochs=10, cutmix=1.0, cutmix_minmax=None, data_path='/scratch/cluster/dilin/datasets/imagenet', data_set='IMNET', decay_epochs=30, decay_rate=0.1, device='cuda', dist_backend='nccl', dist_url='env://', distributed=True, drop=0.0, drop_block=None, drop_path=0.1, epochs=300, eval=False, gpu=0, inat_category='name', input_size=224, lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, min_lr=1e-05, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='deit_tiny_patch16_224', model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, momentum=0.9, num_workers=10, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='/scratch/cluster/cygong/vision_transformer_shuffle', patience_epochs=10, pin_mem=True, rank=0, recount=1, remode='pixel', repeated_aug=True, reprob=0.25, resplit=False, resume='', sched='cosine', seed=0, smoothing=0.1, start_epoch=0, train_interpolation='bicubic', warmup_epochs=5, warmup_lr=1e-06, weight_decay=0.05, world_size=4)
Creating model: deit_tiny_patch16_224
number of params: 5751976
Warning: module PatchEmbed is treated as a zero-op.
Warning: module Dropout is treated as a zero-op.
Warning: module ShuffleData is treated as a zero-op.
Warning: module LayerNorm is treated as a zero-op.
Warning: module Attention is treated as a zero-op.
Warning: module Identity is treated as a zero-op.
Warning: module GELU is treated as a zero-op.
Warning: module Mlp is treated as a zero-op.
Warning: module Block is treated as a zero-op.
Warning: module DropPath is treated as a zero-op.
Warning: module VisionTransformer is treated as a zero-op.
Warning: module DistributedDataParallel is treated as a zero-op.
DistributedDataParallel(
  5.695 M, 99.012% Params, 1.079 GMac, 100.000% MACs, 
  (module): VisionTransformer(
    5.695 M, 99.012% Params, 1.079 GMac, 100.000% MACs, 
    (patch_embed): PatchEmbed(
      0.148 M, 2.567% Params, 0.029 GMac, 2.681% MACs, 
      (proj): Conv2d(0.148 M, 2.567% Params, 0.029 GMac, 2.681% MACs, 3, 192, kernel_size=(16, 16), stride=(16, 16))
    )
    (pos_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
    (blocks): ModuleList(
      5.354 M, 93.090% Params, 1.05 GMac, 97.301% MACs, 
      (0): Sequential(
        0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
        (0): ShuffleData(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
        (1): Block(
          0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            0.148 M, 2.577% Params, 0.029 GMac, 2.691% MACs, 
            (qkv): Linear(0.111 M, 1.933% Params, 0.022 GMac, 2.018% MACs, in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
            (proj): Linear(0.037 M, 0.644% Params, 0.007 GMac, 0.673% MACs, in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (drop_path): Identity(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_post): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp1): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_pre1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (conv): Conv2d(0.002 M, 0.033% Params, 0.0 GMac, 0.035% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        )
      )
      (1): Sequential(
        0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
        (0): ShuffleData(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
        (1): Block(
          0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            0.148 M, 2.577% Params, 0.029 GMac, 2.691% MACs, 
            (qkv): Linear(0.111 M, 1.933% Params, 0.022 GMac, 2.018% MACs, in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
            (proj): Linear(0.037 M, 0.644% Params, 0.007 GMac, 0.673% MACs, in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (drop_path): DropPath(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_post): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp1): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_pre1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (conv): Conv2d(0.002 M, 0.033% Params, 0.0 GMac, 0.035% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        )
      )
      (2): Sequential(
        0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
        (0): ShuffleData(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
        (1): Block(
          0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            0.148 M, 2.577% Params, 0.029 GMac, 2.691% MACs, 
            (qkv): Linear(0.111 M, 1.933% Params, 0.022 GMac, 2.018% MACs, in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
            (proj): Linear(0.037 M, 0.644% Params, 0.007 GMac, 0.673% MACs, in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (drop_path): DropPath(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_post): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp1): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_pre1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (conv): Conv2d(0.002 M, 0.033% Params, 0.0 GMac, 0.035% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        )
      )
      (3): Sequential(
        0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
        (0): ShuffleData(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
        (1): Block(
          0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            0.148 M, 2.577% Params, 0.029 GMac, 2.691% MACs, 
            (qkv): Linear(0.111 M, 1.933% Params, 0.022 GMac, 2.018% MACs, in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
            (proj): Linear(0.037 M, 0.644% Params, 0.007 GMac, 0.673% MACs, in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (drop_path): DropPath(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_post): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp1): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_pre1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (conv): Conv2d(0.002 M, 0.033% Params, 0.0 GMac, 0.035% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        )
      )
      (4): Sequential(
        0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
        (0): ShuffleData(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
        (1): Block(
          0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            0.148 M, 2.577% Params, 0.029 GMac, 2.691% MACs, 
            (qkv): Linear(0.111 M, 1.933% Params, 0.022 GMac, 2.018% MACs, in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
            (proj): Linear(0.037 M, 0.644% Params, 0.007 GMac, 0.673% MACs, in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (drop_path): DropPath(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_post): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp1): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_pre1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (conv): Conv2d(0.002 M, 0.033% Params, 0.0 GMac, 0.035% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        )
      )
      (5): Sequential(
        0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
        (0): ShuffleData(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
        (1): Block(
          0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            0.148 M, 2.577% Params, 0.029 GMac, 2.691% MACs, 
            (qkv): Linear(0.111 M, 1.933% Params, 0.022 GMac, 2.018% MACs, in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
            (proj): Linear(0.037 M, 0.644% Params, 0.007 GMac, 0.673% MACs, in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (drop_path): DropPath(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_post): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp1): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_pre1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (conv): Conv2d(0.002 M, 0.033% Params, 0.0 GMac, 0.035% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        )
      )
      (6): Sequential(
        0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
        (0): ShuffleData(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
        (1): Block(
          0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            0.148 M, 2.577% Params, 0.029 GMac, 2.691% MACs, 
            (qkv): Linear(0.111 M, 1.933% Params, 0.022 GMac, 2.018% MACs, in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
            (proj): Linear(0.037 M, 0.644% Params, 0.007 GMac, 0.673% MACs, in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (drop_path): DropPath(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_post): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp1): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_pre1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (conv): Conv2d(0.002 M, 0.033% Params, 0.0 GMac, 0.035% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        )
      )
      (7): Sequential(
        0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
        (0): ShuffleData(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
        (1): Block(
          0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            0.148 M, 2.577% Params, 0.029 GMac, 2.691% MACs, 
            (qkv): Linear(0.111 M, 1.933% Params, 0.022 GMac, 2.018% MACs, in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
            (proj): Linear(0.037 M, 0.644% Params, 0.007 GMac, 0.673% MACs, in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (drop_path): DropPath(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_post): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp1): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_pre1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (conv): Conv2d(0.002 M, 0.033% Params, 0.0 GMac, 0.035% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        )
      )
      (8): Sequential(
        0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
        (0): ShuffleData(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
        (1): Block(
          0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            0.148 M, 2.577% Params, 0.029 GMac, 2.691% MACs, 
            (qkv): Linear(0.111 M, 1.933% Params, 0.022 GMac, 2.018% MACs, in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
            (proj): Linear(0.037 M, 0.644% Params, 0.007 GMac, 0.673% MACs, in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (drop_path): DropPath(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_post): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp1): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_pre1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (conv): Conv2d(0.002 M, 0.033% Params, 0.0 GMac, 0.035% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        )
      )
      (9): Sequential(
        0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
        (0): ShuffleData(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
        (1): Block(
          0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            0.148 M, 2.577% Params, 0.029 GMac, 2.691% MACs, 
            (qkv): Linear(0.111 M, 1.933% Params, 0.022 GMac, 2.018% MACs, in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
            (proj): Linear(0.037 M, 0.644% Params, 0.007 GMac, 0.673% MACs, in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (drop_path): DropPath(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_post): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp1): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_pre1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (conv): Conv2d(0.002 M, 0.033% Params, 0.0 GMac, 0.035% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        )
      )
      (10): Sequential(
        0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
        (0): ShuffleData(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
        (1): Block(
          0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            0.148 M, 2.577% Params, 0.029 GMac, 2.691% MACs, 
            (qkv): Linear(0.111 M, 1.933% Params, 0.022 GMac, 2.018% MACs, in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
            (proj): Linear(0.037 M, 0.644% Params, 0.007 GMac, 0.673% MACs, in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (drop_path): DropPath(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_post): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp1): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_pre1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (conv): Conv2d(0.002 M, 0.033% Params, 0.0 GMac, 0.035% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        )
      )
      (11): Sequential(
        0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
        (0): ShuffleData(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
        (1): Block(
          0.446 M, 7.757% Params, 0.088 GMac, 8.108% MACs, 
          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            0.148 M, 2.577% Params, 0.029 GMac, 2.691% MACs, 
            (qkv): Linear(0.111 M, 1.933% Params, 0.022 GMac, 2.018% MACs, in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
            (proj): Linear(0.037 M, 0.644% Params, 0.007 GMac, 0.673% MACs, in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (drop_path): DropPath(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_post): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (mlp1): Mlp(
            0.148 M, 2.574% Params, 0.029 GMac, 2.691% MACs, 
            (fc1): Linear(0.074 M, 1.288% Params, 0.015 GMac, 1.346% MACs, in_features=192, out_features=384, bias=True)
            (fc2): Linear(0.074 M, 1.285% Params, 0.015 GMac, 1.346% MACs, in_features=384, out_features=192, bias=True)
            (act): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)
          )
          (norm_pre1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
          (conv): Conv2d(0.002 M, 0.033% Params, 0.0 GMac, 0.035% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        )
      )
    )
    (norm): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-06, elementwise_affine=True)
    (head): Linear(0.193 M, 3.355% Params, 0.0 GMac, 0.018% MACs, in_features=192, out_features=1000, bias=True)
  )
)
('1.08 GMac', '5.75 M')
4
Start training
Epoch: [0]  [   0/1251]  eta: 3:40:38  lr: 0.000001  loss: 6.9406 (6.9406)  time: 10.5821  data: 8.0545  max mem: 9397
Epoch: [0]  [ 100/1251]  eta: 0:13:28  lr: 0.000001  loss: 6.9371 (6.9430)  time: 0.5941  data: 0.0004  max mem: 9465
Epoch: [0]  [ 200/1251]  eta: 0:11:21  lr: 0.000001  loss: 6.9411 (6.9434)  time: 0.5957  data: 0.0004  max mem: 9465
Epoch: [0]  [ 300/1251]  eta: 0:09:59  lr: 0.000001  loss: 6.9396 (6.9428)  time: 0.5918  data: 0.0004  max mem: 9465
Epoch: [0]  [ 400/1251]  eta: 0:08:47  lr: 0.000001  loss: 6.9393 (6.9413)  time: 0.5875  data: 0.0004  max mem: 9465
Epoch: [0]  [ 500/1251]  eta: 0:07:41  lr: 0.000001  loss: 6.9307 (6.9404)  time: 0.5905  data: 0.0003  max mem: 9465
Epoch: [0]  [ 600/1251]  eta: 0:06:37  lr: 0.000001  loss: 6.9321 (6.9397)  time: 0.5890  data: 0.0005  max mem: 9465
Epoch: [0]  [ 700/1251]  eta: 0:05:34  lr: 0.000001  loss: 6.9267 (6.9384)  time: 0.5973  data: 0.0004  max mem: 9465
Epoch: [0]  [ 800/1251]  eta: 0:04:32  lr: 0.000001  loss: 6.9328 (6.9375)  time: 0.5881  data: 0.0005  max mem: 9465
Epoch: [0]  [ 900/1251]  eta: 0:03:31  lr: 0.000001  loss: 6.9291 (6.9365)  time: 0.5901  data: 0.0005  max mem: 9465
Epoch: [0]  [1000/1251]  eta: 0:02:30  lr: 0.000001  loss: 6.9237 (6.9355)  time: 0.5902  data: 0.0005  max mem: 9465
Epoch: [0]  [1100/1251]  eta: 0:01:30  lr: 0.000001  loss: 6.9251 (6.9347)  time: 0.5830  data: 0.0004  max mem: 9465
Epoch: [0]  [1200/1251]  eta: 0:00:30  lr: 0.000001  loss: 6.9268 (6.9338)  time: 0.5964  data: 0.0003  max mem: 9465
Epoch: [0]  [1250/1251]  eta: 0:00:00  lr: 0.000001  loss: 6.9250 (6.9334)  time: 0.5275  data: 0.0010  max mem: 9465
Epoch: [0] Total time: 0:12:28 (0.5983 s / it)
Averaged stats: lr: 0.000001  loss: 6.9250 (6.9337)
Test:  [  0/163]  eta: 0:26:50  loss: 6.7844 (6.7844)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 9.8816  data: 9.1992  max mem: 9465
Test:  [ 10/163]  eta: 0:03:23  loss: 6.8953 (6.9015)  acc1: 0.0000 (0.5626)  acc5: 0.0000 (1.0068)  time: 1.3330  data: 1.1323  max mem: 9465
Test:  [ 20/163]  eta: 0:02:22  loss: 6.8953 (6.8952)  acc1: 0.0000 (0.2947)  acc5: 0.0000 (0.8531)  time: 0.5554  data: 0.3992  max mem: 9465
Test:  [ 30/163]  eta: 0:01:57  loss: 6.8851 (6.8914)  acc1: 0.0000 (0.3573)  acc5: 0.0000 (1.0508)  time: 0.6320  data: 0.4757  max mem: 9465
Test:  [ 40/163]  eta: 0:01:39  loss: 6.8976 (6.8982)  acc1: 0.0000 (0.2860)  acc5: 0.0000 (1.0169)  time: 0.6113  data: 0.4587  max mem: 9465
Test:  [ 50/163]  eta: 0:01:27  loss: 6.9070 (6.8963)  acc1: 0.0000 (0.2299)  acc5: 0.0000 (0.8942)  time: 0.6193  data: 0.4644  max mem: 9465
Test:  [ 60/163]  eta: 0:01:18  loss: 6.9105 (6.9013)  acc1: 0.0000 (0.1922)  acc5: 0.0000 (0.7529)  time: 0.6576  data: 0.5026  max mem: 9465
Test:  [ 70/163]  eta: 0:01:08  loss: 6.9410 (6.9070)  acc1: 0.0000 (0.1652)  acc5: 0.0000 (0.7111)  time: 0.6338  data: 0.4805  max mem: 9465
Test:  [ 80/163]  eta: 0:00:59  loss: 6.9242 (6.9074)  acc1: 0.0000 (0.2292)  acc5: 0.0000 (0.7560)  time: 0.5898  data: 0.4353  max mem: 9465
Test:  [ 90/163]  eta: 0:00:53  loss: 6.9211 (6.9096)  acc1: 0.0000 (0.2398)  acc5: 0.0000 (0.7982)  time: 0.6862  data: 0.5301  max mem: 9465
Test:  [100/163]  eta: 0:00:44  loss: 6.9243 (6.9105)  acc1: 0.0000 (0.2258)  acc5: 0.0000 (0.7418)  time: 0.6967  data: 0.5418  max mem: 9465
Test:  [110/163]  eta: 0:00:37  loss: 6.9243 (6.9137)  acc1: 0.0000 (0.2113)  acc5: 0.0000 (0.7630)  time: 0.5752  data: 0.4214  max mem: 9465
Test:  [120/163]  eta: 0:00:29  loss: 6.9296 (6.9154)  acc1: 0.0000 (0.2369)  acc5: 0.3257 (0.8103)  time: 0.5725  data: 0.4162  max mem: 9465
Test:  [130/163]  eta: 0:00:22  loss: 6.9350 (6.9178)  acc1: 0.0000 (0.2188)  acc5: 0.0000 (0.7484)  time: 0.5965  data: 0.4372  max mem: 9465
Test:  [140/163]  eta: 0:00:15  loss: 6.9178 (6.9164)  acc1: 0.0000 (0.2033)  acc5: 0.0000 (0.7323)  time: 0.5968  data: 0.4399  max mem: 9465
Test:  [150/163]  eta: 0:00:08  loss: 6.8968 (6.9153)  acc1: 0.0000 (0.1898)  acc5: 0.0000 (0.7162)  time: 0.5953  data: 0.4428  max mem: 9465
Test:  [160/163]  eta: 0:00:01  loss: 6.8672 (6.9127)  acc1: 0.0000 (0.2023)  acc5: 0.0000 (0.7466)  time: 0.5614  data: 0.4089  max mem: 9465
Test:  [162/163]  eta: 0:00:00  loss: 6.8672 (6.9127)  acc1: 0.0000 (0.2000)  acc5: 0.0000 (0.7520)  time: 0.5252  data: 0.3721  max mem: 9465
Test: Total time: 0:01:48 (0.6687 s / it)
* Acc@1 0.200 Acc@5 0.752 loss 6.913
Accuracy of the network on the 50000 test images: 0.2%
Max accuracy: 0.20%
Epoch: [1]  [   0/1251]  eta: 4:43:23  lr: 0.000001  loss: 6.9235 (6.9235)  time: 13.5920  data: 4.5248  max mem: 9465
Epoch: [1]  [ 100/1251]  eta: 0:13:47  lr: 0.000001  loss: 6.9246 (6.9256)  time: 0.5842  data: 0.0005  max mem: 9465
Epoch: [1]  [ 200/1251]  eta: 0:11:27  lr: 0.000001  loss: 6.9180 (6.9230)  time: 0.5821  data: 0.0004  max mem: 9465
Epoch: [1]  [ 300/1251]  eta: 0:10:01  lr: 0.000001  loss: 6.9229 (6.9228)  time: 0.5905  data: 0.0005  max mem: 9465
Epoch: [1]  [ 400/1251]  eta: 0:08:49  lr: 0.000001  loss: 6.9184 (6.9222)  time: 0.5853  data: 0.0004  max mem: 9465
Epoch: [1]  [ 500/1251]  eta: 0:07:42  lr: 0.000001  loss: 6.9132 (6.9213)  time: 0.5890  data: 0.0004  max mem: 9465
Epoch: [1]  [ 600/1251]  eta: 0:06:37  lr: 0.000001  loss: 6.9203 (6.9210)  time: 0.5875  data: 0.0004  max mem: 9465
Epoch: [1]  [ 700/1251]  eta: 0:05:34  lr: 0.000001  loss: 6.9194 (6.9202)  time: 0.5802  data: 0.0004  max mem: 9465
Epoch: [1]  [ 800/1251]  eta: 0:04:32  lr: 0.000001  loss: 6.9132 (6.9196)  time: 0.5962  data: 0.0004  max mem: 9465
Epoch: [1]  [ 900/1251]  eta: 0:03:31  lr: 0.000001  loss: 6.9129 (6.9193)  time: 0.5837  data: 0.0005  max mem: 9465
Epoch: [1]  [1000/1251]  eta: 0:02:31  lr: 0.000001  loss: 6.9186 (6.9188)  time: 0.5896  data: 0.0004  max mem: 9465
Epoch: [1]  [1100/1251]  eta: 0:01:30  lr: 0.000001  loss: 6.9160 (6.9186)  time: 0.5880  data: 0.0005  max mem: 9465
Epoch: [1]  [1200/1251]  eta: 0:00:30  lr: 0.000001  loss: 6.9114 (6.9180)  time: 0.5844  data: 0.0008  max mem: 9465
Epoch: [1]  [1250/1251]  eta: 0:00:00  lr: 0.000001  loss: 6.9160 (6.9180)  time: 0.5336  data: 0.0008  max mem: 9465
Epoch: [1] Total time: 0:12:28 (0.5984 s / it)
Averaged stats: lr: 0.000001  loss: 6.9160 (6.9177)
Test:  [  0/163]  eta: 0:21:56  loss: 6.7132 (6.7132)  acc1: 0.0000 (0.0000)  acc5: 10.4235 (10.4235)  time: 8.0753  data: 7.9112  max mem: 9465
Test:  [ 10/163]  eta: 0:03:19  loss: 6.8823 (6.8681)  acc1: 0.0000 (0.0592)  acc5: 0.0000 (1.7767)  time: 1.3011  data: 1.1471  max mem: 9465
Test:  [ 20/163]  eta: 0:02:20  loss: 6.8900 (6.8737)  acc1: 0.0000 (0.1551)  acc5: 0.0000 (1.4115)  time: 0.6274  data: 0.4712  max mem: 9465
Test:  [ 30/163]  eta: 0:01:54  loss: 6.8858 (6.8727)  acc1: 0.0000 (0.5674)  acc5: 0.0000 (1.6076)  time: 0.6191  data: 0.4569  max mem: 9465
Test:  [ 40/163]  eta: 0:01:38  loss: 6.8897 (6.8821)  acc1: 0.0000 (0.4687)  acc5: 0.0000 (1.3347)  time: 0.6044  data: 0.4432  max mem: 9465
Test:  [ 50/163]  eta: 0:01:27  loss: 6.9020 (6.8813)  acc1: 0.0000 (0.3768)  acc5: 0.0000 (1.2327)  time: 0.6304  data: 0.4731  max mem: 9465
Test:  [ 60/163]  eta: 0:01:17  loss: 6.8816 (6.8830)  acc1: 0.0000 (0.3257)  acc5: 0.0000 (1.0947)  time: 0.6621  data: 0.5021  max mem: 9465
Test:  [ 70/163]  eta: 0:01:08  loss: 6.8865 (6.8876)  acc1: 0.0000 (0.2936)  acc5: 0.0000 (1.1515)  time: 0.6356  data: 0.4781  max mem: 9465
Test:  [ 80/163]  eta: 0:00:59  loss: 6.8963 (6.8888)  acc1: 0.0000 (0.3378)  acc5: 0.0000 (1.2145)  time: 0.5979  data: 0.4432  max mem: 9465
Test:  [ 90/163]  eta: 0:00:52  loss: 6.8936 (6.8903)  acc1: 0.0000 (0.3078)  acc5: 0.0000 (1.1598)  time: 0.6881  data: 0.5306  max mem: 9465
Test:  [100/163]  eta: 0:00:44  loss: 6.8985 (6.8912)  acc1: 0.0000 (0.2774)  acc5: 0.0000 (1.0933)  time: 0.7076  data: 0.5523  max mem: 9465
Test:  [110/163]  eta: 0:00:37  loss: 6.9136 (6.8945)  acc1: 0.0000 (0.2582)  acc5: 0.0000 (1.0916)  time: 0.5933  data: 0.4403  max mem: 9465
Test:  [120/163]  eta: 0:00:29  loss: 6.9163 (6.8960)  acc1: 0.0000 (0.2827)  acc5: 0.0000 (1.1360)  time: 0.5784  data: 0.4256  max mem: 9465
Test:  [130/163]  eta: 0:00:22  loss: 6.9163 (6.8989)  acc1: 0.0000 (0.2611)  acc5: 0.0000 (1.0493)  time: 0.6129  data: 0.4580  max mem: 9465
Test:  [140/163]  eta: 0:00:15  loss: 6.9046 (6.8981)  acc1: 0.0000 (0.2426)  acc5: 0.0000 (0.9749)  time: 0.6060  data: 0.4511  max mem: 9465
Test:  [150/163]  eta: 0:00:08  loss: 6.8882 (6.8976)  acc1: 0.0000 (0.2265)  acc5: 0.0000 (0.9707)  time: 0.6250  data: 0.4726  max mem: 9465
Test:  [160/163]  eta: 0:00:02  loss: 6.8352 (6.8927)  acc1: 0.0000 (0.2448)  acc5: 0.0000 (1.0480)  time: 0.6004  data: 0.4464  max mem: 9465
Test:  [162/163]  eta: 0:00:00  loss: 6.8232 (6.8924)  acc1: 0.0000 (0.2460)  acc5: 0.0000 (1.0560)  time: 0.5392  data: 0.3862  max mem: 9465
Test: Total time: 0:01:49 (0.6748 s / it)
* Acc@1 0.246 Acc@5 1.056 loss 6.892
Accuracy of the network on the 50000 test images: 0.2%
Max accuracy: 0.25%
Epoch: [2]  [   0/1251]  eta: 4:31:40  lr: 0.000201  loss: 6.9214 (6.9214)  time: 13.0303  data: 5.5085  max mem: 9465
Epoch: [2]  [ 100/1251]  eta: 0:13:45  lr: 0.000201  loss: 6.8824 (6.9032)  time: 0.5923  data: 0.0004  max mem: 9465
Epoch: [2]  [ 200/1251]  eta: 0:11:26  lr: 0.000201  loss: 6.8578 (6.8857)  time: 0.5851  data: 0.0004  max mem: 9465
Epoch: [2]  [ 300/1251]  eta: 0:10:00  lr: 0.000201  loss: 6.8400 (6.8739)  time: 0.5872  data: 0.0004  max mem: 9465
Epoch: [2]  [ 400/1251]  eta: 0:08:48  lr: 0.000201  loss: 6.8249 (6.8613)  time: 0.5854  data: 0.0004  max mem: 9465
Epoch: [2]  [ 500/1251]  eta: 0:07:40  lr: 0.000201  loss: 6.8188 (6.8515)  time: 0.5844  data: 0.0004  max mem: 9465
Epoch: [2]  [ 600/1251]  eta: 0:06:36  lr: 0.000201  loss: 6.7964 (6.8420)  time: 0.5848  data: 0.0004  max mem: 9465
Epoch: [2]  [ 700/1251]  eta: 0:05:34  lr: 0.000201  loss: 6.7702 (6.8321)  time: 0.5873  data: 0.0004  max mem: 9465
Epoch: [2]  [ 800/1251]  eta: 0:04:32  lr: 0.000201  loss: 6.7317 (6.8205)  time: 0.5877  data: 0.0004  max mem: 9465
Epoch: [2]  [ 900/1251]  eta: 0:03:31  lr: 0.000201  loss: 6.7436 (6.8084)  time: 0.5892  data: 0.0004  max mem: 9465
Epoch: [2]  [1000/1251]  eta: 0:02:30  lr: 0.000201  loss: 6.6705 (6.7958)  time: 0.5851  data: 0.0004  max mem: 9465
Epoch: [2]  [1100/1251]  eta: 0:01:30  lr: 0.000201  loss: 6.7087 (6.7833)  time: 0.5867  data: 0.0004  max mem: 9465
Epoch: [2]  [1200/1251]  eta: 0:00:30  lr: 0.000201  loss: 6.6521 (6.7729)  time: 0.5891  data: 0.0004  max mem: 9465
Epoch: [2]  [1250/1251]  eta: 0:00:00  lr: 0.000201  loss: 6.6395 (6.7675)  time: 0.5260  data: 0.0005  max mem: 9465
Epoch: [2] Total time: 0:12:26 (0.5965 s / it)
Averaged stats: lr: 0.000201  loss: 6.6395 (6.7666)
Test:  [  0/163]  eta: 0:22:43  loss: 5.1337 (5.1337)  acc1: 9.7720 (9.7720)  acc5: 42.6710 (42.6710)  time: 8.3647  data: 8.2052  max mem: 9465
Test:  [ 10/163]  eta: 0:03:24  loss: 5.9979 (5.9667)  acc1: 0.9772 (1.7471)  acc5: 8.4691 (10.4235)  time: 1.3356  data: 1.1784  max mem: 9465
Test:  [ 20/163]  eta: 0:02:23  loss: 6.0614 (6.0525)  acc1: 0.6515 (2.0940)  acc5: 6.5147 (8.8568)  time: 0.6345  data: 0.4738  max mem: 9465
Test:  [ 30/163]  eta: 0:01:56  loss: 6.1592 (6.1242)  acc1: 0.3257 (1.9544)  acc5: 3.9088 (8.3850)  time: 0.6274  data: 0.4680  max mem: 9465
Test:  [ 40/163]  eta: 0:01:39  loss: 6.3632 (6.1874)  acc1: 0.3257 (1.7399)  acc5: 3.5831 (7.4283)  time: 0.6077  data: 0.4500  max mem: 9465
Test:  [ 50/163]  eta: 0:01:28  loss: 6.2910 (6.1859)  acc1: 0.6515 (1.8075)  acc5: 4.8860 (7.6515)  time: 0.6319  data: 0.4751  max mem: 9465
Test:  [ 60/163]  eta: 0:01:18  loss: 6.2118 (6.1751)  acc1: 0.3257 (1.7889)  acc5: 4.8860 (7.6734)  time: 0.6640  data: 0.5115  max mem: 9465
Test:  [ 70/163]  eta: 0:01:09  loss: 6.1817 (6.1783)  acc1: 0.3257 (1.8672)  acc5: 6.1889 (7.9736)  time: 0.6633  data: 0.5074  max mem: 9465
Test:  [ 80/163]  eta: 0:01:01  loss: 6.1977 (6.1864)  acc1: 0.9772 (1.9142)  acc5: 6.5147 (8.1353)  time: 0.6987  data: 0.5402  max mem: 9465
Test:  [ 90/163]  eta: 0:00:54  loss: 6.2333 (6.1953)  acc1: 0.9772 (1.9938)  acc5: 5.8632 (8.0145)  time: 0.7376  data: 0.5776  max mem: 9465
Test:  [100/163]  eta: 0:00:47  loss: 6.2292 (6.2027)  acc1: 0.6515 (2.0963)  acc5: 5.8632 (8.0627)  time: 0.7675  data: 0.6060  max mem: 9465
Test:  [110/163]  eta: 0:00:39  loss: 6.1309 (6.2037)  acc1: 1.3029 (2.0512)  acc5: 7.4919 (8.0553)  time: 0.7721  data: 0.6071  max mem: 9465
Test:  [120/163]  eta: 0:00:32  loss: 6.2683 (6.2128)  acc1: 1.3029 (2.0352)  acc5: 6.5147 (7.8230)  time: 0.7497  data: 0.5745  max mem: 9465
Test:  [130/163]  eta: 0:00:24  loss: 6.3148 (6.2168)  acc1: 0.6515 (2.0663)  acc5: 5.2117 (7.8250)  time: 0.7043  data: 0.5342  max mem: 9465
Test:  [140/163]  eta: 0:00:16  loss: 6.2642 (6.2220)  acc1: 0.9772 (2.0930)  acc5: 5.2117 (7.7298)  time: 0.6410  data: 0.4844  max mem: 9465
Test:  [150/163]  eta: 0:00:09  loss: 6.1479 (6.2196)  acc1: 1.3029 (2.1076)  acc5: 6.8404 (7.8758)  time: 0.6192  data: 0.4597  max mem: 9465
Test:  [160/163]  eta: 0:00:02  loss: 5.9881 (6.1770)  acc1: 4.5603 (2.7353)  acc5: 14.3322 (9.0780)  time: 0.4960  data: 0.3375  max mem: 9465
Test:  [162/163]  eta: 0:00:00  loss: 5.9292 (6.1736)  acc1: 4.5603 (2.7240)  acc5: 14.3322 (9.1360)  time: 0.4892  data: 0.3316  max mem: 9465
Test: Total time: 0:01:54 (0.7047 s / it)
* Acc@1 2.724 Acc@5 9.136 loss 6.174
Accuracy of the network on the 50000 test images: 2.7%
Max accuracy: 2.72%
Epoch: [3]  [   0/1251]  eta: 2:44:23  lr: 0.000401  loss: 6.6063 (6.6063)  time: 7.8848  data: 7.2249  max mem: 9465
Epoch: [3]  [ 100/1251]  eta: 0:12:50  lr: 0.000401  loss: 6.6741 (6.6407)  time: 0.5922  data: 0.0004  max mem: 9465
Epoch: [3]  [ 200/1251]  eta: 0:11:02  lr: 0.000401  loss: 6.5533 (6.6135)  time: 0.5907  data: 0.0004  max mem: 9465
Epoch: [3]  [ 300/1251]  eta: 0:09:45  lr: 0.000401  loss: 6.6612 (6.6109)  time: 0.5844  data: 0.0004  max mem: 9465
Epoch: [3]  [ 400/1251]  eta: 0:08:38  lr: 0.000401  loss: 6.5580 (6.5948)  time: 0.5931  data: 0.0004  max mem: 9465
Epoch: [3]  [ 500/1251]  eta: 0:07:34  lr: 0.000401  loss: 6.5958 (6.5899)  time: 0.5850  data: 0.0004  max mem: 9465
Epoch: [3]  [ 600/1251]  eta: 0:06:32  lr: 0.000401  loss: 6.5320 (6.5797)  time: 0.5891  data: 0.0004  max mem: 9465
Epoch: [3]  [ 700/1251]  eta: 0:05:30  lr: 0.000401  loss: 6.5805 (6.5713)  time: 0.5819  data: 0.0004  max mem: 9465
Epoch: [3]  [ 800/1251]  eta: 0:04:30  lr: 0.000401  loss: 6.5183 (6.5627)  time: 0.5869  data: 0.0004  max mem: 9465
Epoch: [3]  [ 900/1251]  eta: 0:03:29  lr: 0.000401  loss: 6.5159 (6.5553)  time: 0.5884  data: 0.0004  max mem: 9465
Epoch: [3]  [1000/1251]  eta: 0:02:29  lr: 0.000401  loss: 6.4767 (6.5491)  time: 0.5842  data: 0.0004  max mem: 9465
Epoch: [3]  [1100/1251]  eta: 0:01:29  lr: 0.000401  loss: 6.5317 (6.5408)  time: 0.5794  data: 0.0004  max mem: 9465
Epoch: [3]  [1200/1251]  eta: 0:00:30  lr: 0.000401  loss: 6.4657 (6.5311)  time: 0.5868  data: 0.0004  max mem: 9465
Epoch: [3]  [1250/1251]  eta: 0:00:00  lr: 0.000401  loss: 6.5294 (6.5290)  time: 0.5277  data: 0.0008  max mem: 9465
Epoch: [3] Total time: 0:12:23 (0.5944 s / it)
Averaged stats: lr: 0.000401  loss: 6.5294 (6.5309)
Test:  [  0/163]  eta: 0:22:48  loss: 4.7764 (4.7764)  acc1: 14.3322 (14.3322)  acc5: 46.2541 (46.2541)  time: 8.3978  data: 8.2373  max mem: 9465
Test:  [ 10/163]  eta: 0:03:25  loss: 5.4455 (5.3793)  acc1: 5.8632 (8.0545)  acc5: 18.8925 (22.6532)  time: 1.3428  data: 1.1859  max mem: 9465
Test:  [ 20/163]  eta: 0:02:23  loss: 5.4455 (5.4442)  acc1: 5.2117 (7.3212)  acc5: 18.5668 (20.3195)  time: 0.6374  data: 0.4790  max mem: 9465
Test:  [ 30/163]  eta: 0:01:57  loss: 5.4857 (5.5187)  acc1: 5.2117 (6.8719)  acc5: 18.5668 (19.4599)  time: 0.6279  data: 0.4714  max mem: 9465
Test:  [ 40/163]  eta: 0:01:39  loss: 5.7104 (5.5756)  acc1: 3.9088 (6.3081)  acc5: 14.0065 (18.3125)  time: 0.6056  data: 0.4530  max mem: 9465
Test:  [ 50/163]  eta: 0:01:28  loss: 5.6939 (5.5854)  acc1: 3.2573 (5.8568)  acc5: 14.6580 (17.7429)  time: 0.6245  data: 0.4696  max mem: 9465
Test:  [ 60/163]  eta: 0:01:18  loss: 5.6181 (5.5764)  acc1: 3.2573 (6.0287)  acc5: 15.6352 (18.0274)  time: 0.6562  data: 0.5012  max mem: 9465
Test:  [ 70/163]  eta: 0:01:08  loss: 5.6620 (5.5968)  acc1: 3.9088 (5.9182)  acc5: 16.2866 (17.7089)  time: 0.6325  data: 0.4792  max mem: 9465
Test:  [ 80/163]  eta: 0:01:00  loss: 5.7295 (5.6186)  acc1: 3.5831 (5.6420)  acc5: 15.3094 (17.4569)  time: 0.6545  data: 0.4986  max mem: 9465
Test:  [ 90/163]  eta: 0:00:52  loss: 5.7941 (5.6318)  acc1: 3.2573 (5.6592)  acc5: 13.6808 (17.1851)  time: 0.6729  data: 0.5176  max mem: 9465
Test:  [100/163]  eta: 0:00:45  loss: 5.7222 (5.6508)  acc1: 3.2573 (5.6116)  acc5: 11.7264 (16.8027)  time: 0.6745  data: 0.5193  max mem: 9465
Test:  [110/163]  eta: 0:00:38  loss: 5.6792 (5.6536)  acc1: 3.5831 (5.6108)  acc5: 14.0065 (16.8031)  time: 0.7041  data: 0.5482  max mem: 9465
Test:  [120/163]  eta: 0:00:30  loss: 5.7149 (5.6688)  acc1: 3.5831 (5.6398)  acc5: 14.6580 (16.5774)  time: 0.6760  data: 0.5204  max mem: 9465
Test:  [130/163]  eta: 0:00:23  loss: 5.7929 (5.6765)  acc1: 3.9088 (5.6344)  acc5: 13.3550 (16.5577)  time: 0.6671  data: 0.5123  max mem: 9465
Test:  [140/163]  eta: 0:00:16  loss: 5.8097 (5.6890)  acc1: 2.2801 (5.4358)  acc5: 12.0521 (16.0903)  time: 0.6240  data: 0.4704  max mem: 9465
Test:  [150/163]  eta: 0:00:09  loss: 5.6749 (5.6867)  acc1: 3.2573 (5.4447)  acc5: 12.0521 (16.2155)  time: 0.6247  data: 0.4673  max mem: 9465
Test:  [160/163]  eta: 0:00:02  loss: 5.4864 (5.6379)  acc1: 6.8404 (6.1100)  acc5: 20.8469 (17.4560)  time: 0.5929  data: 0.4366  max mem: 9465
Test:  [162/163]  eta: 0:00:00  loss: 5.1936 (5.6327)  acc1: 7.1661 (6.2100)  acc5: 20.8469 (17.5780)  time: 0.5028  data: 0.3479  max mem: 9465
Test: Total time: 0:01:52 (0.6926 s / it)
* Acc@1 6.210 Acc@5 17.578 loss 5.633
Accuracy of the network on the 50000 test images: 6.2%
Max accuracy: 6.21%
Epoch: [4]  [   0/1251]  eta: 3:17:24  lr: 0.000600  loss: 6.6540 (6.6540)  time: 9.4679  data: 6.4312  max mem: 9465
Epoch: [4]  [ 100/1251]  eta: 0:13:08  lr: 0.000600  loss: 6.4992 (6.4321)  time: 0.5841  data: 0.0004  max mem: 9465
Epoch: [4]  [ 200/1251]  eta: 0:11:12  lr: 0.000600  loss: 6.4883 (6.4430)  time: 0.6019  data: 0.0004  max mem: 9465
Epoch: [4]  [ 300/1251]  eta: 0:09:51  lr: 0.000600  loss: 6.4638 (6.4469)  time: 0.5885  data: 0.0003  max mem: 9465
Epoch: [4]  [ 400/1251]  eta: 0:08:43  lr: 0.000600  loss: 6.5146 (6.4388)  time: 0.6086  data: 0.0004  max mem: 9465
Epoch: [4]  [ 500/1251]  eta: 0:07:40  lr: 0.000600  loss: 6.4683 (6.4330)  time: 0.6339  data: 0.0004  max mem: 9465
Epoch: [4]  [ 600/1251]  eta: 0:06:41  lr: 0.000600  loss: 6.4374 (6.4212)  time: 0.5988  data: 0.0004  max mem: 9465
Epoch: [4]  [ 700/1251]  eta: 0:05:37  lr: 0.000600  loss: 6.4602 (6.4162)  time: 0.5776  data: 0.0004  max mem: 9465
Epoch: [4]  [ 800/1251]  eta: 0:04:35  lr: 0.000600  loss: 6.3401 (6.4060)  time: 0.5891  data: 0.0004  max mem: 9465
Epoch: [4]  [ 900/1251]  eta: 0:03:33  lr: 0.000600  loss: 6.1657 (6.4018)  time: 0.5903  data: 0.0004  max mem: 9465
Epoch: [4]  [1000/1251]  eta: 0:02:31  lr: 0.000600  loss: 6.3906 (6.3980)  time: 0.5942  data: 0.0004  max mem: 9465
Epoch: [4]  [1100/1251]  eta: 0:01:31  lr: 0.000600  loss: 6.3791 (6.3910)  time: 0.5903  data: 0.0004  max mem: 9465
Epoch: [4]  [1200/1251]  eta: 0:00:30  lr: 0.000600  loss: 6.4304 (6.3819)  time: 0.6107  data: 0.0004  max mem: 9465
Epoch: [4]  [1250/1251]  eta: 0:00:00  lr: 0.000600  loss: 6.3741 (6.3783)  time: 0.6409  data: 0.0294  max mem: 9465
Epoch: [4] Total time: 0:12:36 (0.6049 s / it)
Averaged stats: lr: 0.000600  loss: 6.3741 (6.3714)
Test:  [  0/163]  eta: 0:23:57  loss: 4.2465 (4.2465)  acc1: 16.6124 (16.6124)  acc5: 52.4430 (52.4430)  time: 8.8163  data: 8.6562  max mem: 9465
Test:  [ 10/163]  eta: 0:03:27  loss: 5.0865 (4.9843)  acc1: 6.5147 (9.1205)  acc5: 22.8013 (26.7397)  time: 1.3549  data: 1.1967  max mem: 9465
Test:  [ 20/163]  eta: 0:02:25  loss: 4.9464 (4.9252)  acc1: 8.7948 (11.6643)  acc5: 26.3844 (29.3160)  time: 0.6239  data: 0.4687  max mem: 9465
Test:  [ 30/163]  eta: 0:01:57  loss: 4.8758 (4.9110)  acc1: 11.0749 (11.5898)  acc5: 27.3616 (28.9797)  time: 0.6256  data: 0.4701  max mem: 9465
Test:  [ 40/163]  eta: 0:01:40  loss: 4.9303 (4.9202)  acc1: 6.5147 (10.7651)  acc5: 24.1042 (28.3308)  time: 0.6062  data: 0.4452  max mem: 9465
Test:  [ 50/163]  eta: 0:01:28  loss: 4.9303 (4.9233)  acc1: 6.8404 (10.4362)  acc5: 24.1042 (28.0897)  time: 0.6233  data: 0.4633  max mem: 9465
Test:  [ 60/163]  eta: 0:01:18  loss: 4.8621 (4.9130)  acc1: 8.1433 (10.7705)  acc5: 26.3844 (28.2533)  time: 0.6563  data: 0.4987  max mem: 9465
Test:  [ 70/163]  eta: 0:01:11  loss: 4.9548 (4.9544)  acc1: 8.1433 (10.6024)  acc5: 25.0814 (27.8983)  time: 0.7306  data: 0.5699  max mem: 9465
Test:  [ 80/163]  eta: 0:01:02  loss: 5.3934 (5.0224)  acc1: 7.8176 (10.2666)  acc5: 20.8469 (27.0801)  time: 0.7097  data: 0.5449  max mem: 9465
Test:  [ 90/163]  eta: 0:00:54  loss: 5.5242 (5.0785)  acc1: 7.1661 (10.0762)  acc5: 20.8469 (26.2412)  time: 0.6578  data: 0.4981  max mem: 9465
Test:  [100/163]  eta: 0:00:47  loss: 5.5577 (5.1266)  acc1: 5.5375 (9.8204)  acc5: 18.2410 (25.4975)  time: 0.7397  data: 0.5817  max mem: 9465
Test:  [110/163]  eta: 0:00:39  loss: 5.4767 (5.1569)  acc1: 5.8632 (9.5548)  acc5: 18.2410 (24.9758)  time: 0.7669  data: 0.6011  max mem: 9465
Test:  [120/163]  eta: 0:00:31  loss: 5.4983 (5.1939)  acc1: 5.8632 (9.3332)  acc5: 17.9153 (24.3896)  time: 0.7222  data: 0.5586  max mem: 9465
Test:  [130/163]  eta: 0:00:24  loss: 5.5740 (5.2237)  acc1: 5.8632 (9.2150)  acc5: 16.6124 (23.9476)  time: 0.6920  data: 0.5339  max mem: 9465
Test:  [140/163]  eta: 0:00:16  loss: 5.6432 (5.2562)  acc1: 3.9088 (8.9011)  acc5: 15.3094 (23.1940)  time: 0.6630  data: 0.5048  max mem: 9465
Test:  [150/163]  eta: 0:00:09  loss: 5.4917 (5.2627)  acc1: 4.8860 (8.9350)  acc5: 16.9381 (23.1551)  time: 0.6721  data: 0.5103  max mem: 9465
Test:  [160/163]  eta: 0:00:02  loss: 4.7971 (5.2034)  acc1: 9.7720 (9.6971)  acc5: 33.2248 (24.5473)  time: 0.5449  data: 0.3864  max mem: 9465
Test:  [162/163]  eta: 0:00:00  loss: 4.6645 (5.1935)  acc1: 13.6808 (9.9060)  acc5: 34.8534 (24.8260)  time: 0.4573  data: 0.3024  max mem: 9465
Test: Total time: 0:01:55 (0.7111 s / it)
* Acc@1 9.906 Acc@5 24.826 loss 5.193
Accuracy of the network on the 50000 test images: 9.9%
Max accuracy: 9.91%
Epoch: [5]  [   0/1251]  eta: 2:46:17  lr: 0.000800  loss: 6.4886 (6.4886)  time: 7.9753  data: 7.2087  max mem: 9465
Epoch: [5]  [ 100/1251]  eta: 0:12:52  lr: 0.000800  loss: 6.3630 (6.3449)  time: 0.5899  data: 0.0004  max mem: 9465
Epoch: [5]  [ 200/1251]  eta: 0:11:04  lr: 0.000800  loss: 6.2984 (6.3226)  time: 0.5976  data: 0.0004  max mem: 9465
Epoch: [5]  [ 300/1251]  eta: 0:09:54  lr: 0.000800  loss: 6.3988 (6.3036)  time: 0.5995  data: 0.0004  max mem: 9465
Epoch: [5]  [ 400/1251]  eta: 0:08:51  lr: 0.000800  loss: 6.2193 (6.2905)  time: 0.6071  data: 0.0008  max mem: 9465
Epoch: [5]  [ 500/1251]  eta: 0:07:45  lr: 0.000800  loss: 6.0628 (6.2815)  time: 0.5911  data: 0.0004  max mem: 9465
Epoch: [5]  [ 600/1251]  eta: 0:06:39  lr: 0.000800  loss: 6.0326 (6.2690)  time: 0.5883  data: 0.0004  max mem: 9465
Epoch: [5]  [ 700/1251]  eta: 0:05:36  lr: 0.000800  loss: 6.1223 (6.2547)  time: 0.5910  data: 0.0004  max mem: 9465
Epoch: [5]  [ 800/1251]  eta: 0:04:33  lr: 0.000800  loss: 6.2396 (6.2435)  time: 0.5883  data: 0.0004  max mem: 9465
Epoch: [5]  [ 900/1251]  eta: 0:03:32  lr: 0.000800  loss: 6.3440 (6.2379)  time: 0.6122  data: 0.0004  max mem: 9465
Epoch: [5]  [1000/1251]  eta: 0:02:32  lr: 0.000800  loss: 6.2665 (6.2317)  time: 0.6618  data: 0.0004  max mem: 9465
Epoch: [5]  [1100/1251]  eta: 0:01:32  lr: 0.000800  loss: 6.2629 (6.2251)  time: 0.6578  data: 0.0004  max mem: 9465
Epoch: [5]  [1200/1251]  eta: 0:00:31  lr: 0.000800  loss: 6.0293 (6.2154)  time: 0.6501  data: 0.0004  max mem: 9465
Epoch: [5]  [1250/1251]  eta: 0:00:00  lr: 0.000800  loss: 6.1625 (6.2120)  time: 0.5697  data: 0.0011  max mem: 9465
Epoch: [5] Total time: 0:12:46 (0.6127 s / it)
Averaged stats: lr: 0.000800  loss: 6.1625 (6.2177)
Test:  [  0/163]  eta: 0:24:42  loss: 3.7778 (3.7778)  acc1: 27.6873 (27.6873)  acc5: 59.2834 (59.2834)  time: 9.0969  data: 8.9380  max mem: 9465
Test:  [ 10/163]  eta: 0:03:49  loss: 4.4503 (4.4396)  acc1: 14.3322 (15.9609)  acc5: 35.1792 (37.6962)  time: 1.4978  data: 1.3405  max mem: 9465
Test:  [ 20/163]  eta: 0:02:40  loss: 4.2643 (4.3659)  acc1: 17.2638 (18.4427)  acc5: 42.0195 (40.3909)  time: 0.7253  data: 0.5658  max mem: 9465
Test:  [ 30/163]  eta: 0:02:12  loss: 4.2643 (4.4048)  acc1: 17.2638 (17.1483)  acc5: 42.0195 (39.3611)  time: 0.7182  data: 0.5610  max mem: 9465
Test:  [ 40/163]  eta: 0:01:53  loss: 4.6421 (4.4600)  acc1: 10.7492 (15.4366)  acc5: 29.3160 (37.5943)  time: 0.7189  data: 0.5665  max mem: 9465
Test:  [ 50/163]  eta: 0:01:40  loss: 4.6260 (4.4698)  acc1: 11.4007 (15.3925)  acc5: 31.9218 (37.4210)  time: 0.7316  data: 0.5792  max mem: 9465
Test:  [ 60/163]  eta: 0:01:29  loss: 4.5282 (4.4638)  acc1: 12.0521 (15.5070)  acc5: 35.5049 (37.4646)  time: 0.7640  data: 0.6089  max mem: 9465
Test:  [ 70/163]  eta: 0:01:19  loss: 4.4185 (4.4922)  acc1: 11.7264 (15.5067)  acc5: 35.5049 (37.0143)  time: 0.7489  data: 0.5928  max mem: 9465
Test:  [ 80/163]  eta: 0:01:08  loss: 4.9684 (4.5475)  acc1: 10.0977 (15.0038)  acc5: 28.6645 (36.0277)  time: 0.6998  data: 0.5388  max mem: 9465
Test:  [ 90/163]  eta: 0:01:00  loss: 4.9684 (4.5786)  acc1: 10.0977 (14.9801)  acc5: 28.6645 (35.5085)  time: 0.7748  data: 0.6144  max mem: 9465
Test:  [100/163]  eta: 0:00:51  loss: 4.8787 (4.6147)  acc1: 12.0521 (14.7709)  acc5: 30.2932 (34.9631)  time: 0.7671  data: 0.6085  max mem: 9465
Test:  [110/163]  eta: 0:00:42  loss: 4.7307 (4.6348)  acc1: 10.0977 (14.5318)  acc5: 29.6417 (34.5189)  time: 0.6747  data: 0.5134  max mem: 9465
Test:  [120/163]  eta: 0:00:34  loss: 4.9337 (4.6711)  acc1: 9.4463 (14.2192)  acc5: 28.3388 (33.8089)  time: 0.6720  data: 0.5143  max mem: 9465
Test:  [130/163]  eta: 0:00:25  loss: 5.0405 (4.6884)  acc1: 10.0977 (14.0836)  acc5: 25.4072 (33.3640)  time: 0.6737  data: 0.5183  max mem: 9465
Test:  [140/163]  eta: 0:00:17  loss: 5.0905 (4.7165)  acc1: 8.1433 (13.7455)  acc5: 21.4984 (32.6680)  time: 0.6576  data: 0.4960  max mem: 9465
Test:  [150/163]  eta: 0:00:10  loss: 4.7768 (4.7189)  acc1: 11.4007 (13.7822)  acc5: 27.6873 (32.7092)  time: 0.6821  data: 0.5125  max mem: 9465
Test:  [160/163]  eta: 0:00:02  loss: 4.4145 (4.6744)  acc1: 18.8925 (14.5588)  acc5: 39.7394 (33.8762)  time: 0.6124  data: 0.4497  max mem: 9465
Test:  [162/163]  eta: 0:00:00  loss: 4.4145 (4.6643)  acc1: 18.8925 (14.8080)  acc5: 40.7166 (34.1380)  time: 0.4667  data: 0.3050  max mem: 9465
Test: Total time: 0:02:04 (0.7648 s / it)
* Acc@1 14.808 Acc@5 34.138 loss 4.664
Accuracy of the network on the 50000 test images: 14.8%
Max accuracy: 14.81%
Epoch: [6]  [   0/1251]  eta: 3:21:26  lr: 0.000999  loss: 5.7485 (5.7485)  time: 9.6613  data: 8.1099  max mem: 9465
Epoch: [6]  [ 100/1251]  eta: 0:14:54  lr: 0.000999  loss: 6.0795 (6.0880)  time: 0.7004  data: 0.0005  max mem: 9465
Epoch: [6]  [ 200/1251]  eta: 0:12:38  lr: 0.000999  loss: 6.2387 (6.1176)  time: 0.6786  data: 0.0006  max mem: 9465
Epoch: [6]  [ 300/1251]  eta: 0:11:07  lr: 0.000999  loss: 6.1434 (6.1000)  time: 0.6300  data: 0.0005  max mem: 9465
Epoch: [6]  [ 400/1251]  eta: 0:09:50  lr: 0.000999  loss: 6.2982 (6.1074)  time: 0.6362  data: 0.0004  max mem: 9465
Epoch: [6]  [ 500/1251]  eta: 0:08:39  lr: 0.000999  loss: 6.0877 (6.0935)  time: 0.6674  data: 0.0005  max mem: 9465
Epoch: [6]  [ 600/1251]  eta: 0:07:27  lr: 0.000999  loss: 6.2493 (6.0844)  time: 0.6580  data: 0.0004  max mem: 9465
Epoch: [6]  [ 700/1251]  eta: 0:06:18  lr: 0.000999  loss: 6.0782 (6.0763)  time: 0.6942  data: 0.0006  max mem: 9465
Epoch: [6]  [ 800/1251]  eta: 0:05:08  lr: 0.000999  loss: 5.9975 (6.0687)  time: 0.6396  data: 0.0004  max mem: 9465
Epoch: [6]  [ 900/1251]  eta: 0:03:59  lr: 0.000999  loss: 5.9602 (6.0626)  time: 0.6590  data: 0.0012  max mem: 9465
Epoch: [6]  [1000/1251]  eta: 0:02:50  lr: 0.000999  loss: 6.1746 (6.0567)  time: 0.6585  data: 0.0004  max mem: 9465
Epoch: [6]  [1100/1251]  eta: 0:01:42  lr: 0.000999  loss: 6.1153 (6.0532)  time: 0.7314  data: 0.0012  max mem: 9465
Epoch: [6]  [1200/1251]  eta: 0:00:34  lr: 0.000999  loss: 5.9647 (6.0437)  time: 0.6559  data: 0.0004  max mem: 9465
Epoch: [6]  [1250/1251]  eta: 0:00:00  lr: 0.000999  loss: 6.0671 (6.0426)  time: 0.5435  data: 0.0014  max mem: 9465
Epoch: [6] Total time: 0:14:06 (0.6770 s / it)
Averaged stats: lr: 0.000999  loss: 6.0671 (6.0459)
Test:  [  0/163]  eta: 0:25:27  loss: 3.4363 (3.4363)  acc1: 31.5961 (31.5961)  acc5: 66.4495 (66.4495)  time: 9.3726  data: 9.1993  max mem: 9465
Test:  [ 10/163]  eta: 0:04:03  loss: 4.0335 (4.0318)  acc1: 20.5212 (21.7649)  acc5: 44.6254 (46.3429)  time: 1.5915  data: 1.4363  max mem: 9465
Test:  [ 20/163]  eta: 0:02:51  loss: 3.9581 (3.9621)  acc1: 21.1726 (23.8716)  acc5: 47.2313 (48.1619)  time: 0.7933  data: 0.6403  max mem: 9465
Test:  [ 30/163]  eta: 0:02:20  loss: 3.9581 (3.9864)  acc1: 21.1726 (22.9169)  acc5: 47.2313 (46.7900)  time: 0.7632  data: 0.6082  max mem: 9465
Test:  [ 40/163]  eta: 0:02:00  loss: 4.2005 (4.0250)  acc1: 15.3094 (20.9979)  acc5: 38.7622 (45.7694)  time: 0.7507  data: 0.5957  max mem: 9465
Test:  [ 50/163]  eta: 0:01:47  loss: 4.0904 (4.0220)  acc1: 15.3094 (20.9299)  acc5: 41.3681 (45.7495)  time: 0.7845  data: 0.6289  max mem: 9465
Test:  [ 60/163]  eta: 0:01:35  loss: 4.0619 (4.0296)  acc1: 18.5668 (21.1833)  acc5: 44.2997 (45.5652)  time: 0.8117  data: 0.6523  max mem: 9465
Test:  [ 70/163]  eta: 0:01:23  loss: 4.0975 (4.0587)  acc1: 18.5668 (21.2506)  acc5: 44.2997 (45.2998)  time: 0.7674  data: 0.6109  max mem: 9465
Test:  [ 80/163]  eta: 0:01:12  loss: 4.5671 (4.1309)  acc1: 16.9381 (20.5413)  acc5: 36.8078 (44.0463)  time: 0.7192  data: 0.5659  max mem: 9465
Test:  [ 90/163]  eta: 0:01:04  loss: 4.6256 (4.1808)  acc1: 15.3094 (20.1131)  acc5: 33.8762 (43.1507)  time: 0.8120  data: 0.6524  max mem: 9465
Test:  [100/163]  eta: 0:00:54  loss: 4.5445 (4.2213)  acc1: 16.2866 (19.9665)  acc5: 35.8306 (42.6130)  time: 0.7994  data: 0.6396  max mem: 9465
Test:  [110/163]  eta: 0:00:44  loss: 4.4737 (4.2520)  acc1: 14.6580 (19.5997)  acc5: 36.8078 (42.0195)  time: 0.6780  data: 0.5228  max mem: 9465
Test:  [120/163]  eta: 0:00:35  loss: 4.5521 (4.2888)  acc1: 14.6580 (19.2613)  acc5: 35.8306 (41.3438)  time: 0.7020  data: 0.5458  max mem: 9465
Test:  [130/163]  eta: 0:00:27  loss: 4.7079 (4.3175)  acc1: 15.6352 (19.0740)  acc5: 33.5505 (40.7315)  time: 0.7198  data: 0.5605  max mem: 9465
Test:  [140/163]  eta: 0:00:18  loss: 4.6596 (4.3458)  acc1: 15.6352 (18.6384)  acc5: 30.6189 (40.1576)  time: 0.7012  data: 0.5431  max mem: 9465
Test:  [150/163]  eta: 0:00:10  loss: 4.4954 (4.3488)  acc1: 15.9609 (18.6725)  acc5: 35.1792 (40.1104)  time: 0.6840  data: 0.5278  max mem: 9465
Test:  [160/163]  eta: 0:00:02  loss: 3.8990 (4.3014)  acc1: 23.4528 (19.4206)  acc5: 43.6482 (41.1880)  time: 0.5738  data: 0.4182  max mem: 9465
Test:  [162/163]  eta: 0:00:00  loss: 3.8003 (4.2910)  acc1: 26.6917 (19.6560)  acc5: 49.8371 (41.3940)  time: 0.5319  data: 0.3773  max mem: 9465
Test: Total time: 0:02:08 (0.7871 s / it)
* Acc@1 19.656 Acc@5 41.394 loss 4.291
Accuracy of the network on the 50000 test images: 19.7%
Max accuracy: 19.66%
Epoch: [7]  [   0/1251]  eta: 3:13:15  lr: 0.000999  loss: 6.3505 (6.3505)  time: 9.2687  data: 7.8701  max mem: 9465
Epoch: [7]  [ 100/1251]  eta: 0:14:08  lr: 0.000999  loss: 6.0664 (5.9720)  time: 0.6629  data: 0.0499  max mem: 9465
Epoch: [7]  [ 200/1251]  eta: 0:12:02  lr: 0.000999  loss: 5.7982 (5.9383)  time: 0.6484  data: 0.0550  max mem: 9465
Epoch: [7]  [ 300/1251]  eta: 0:10:41  lr: 0.000999  loss: 5.6683 (5.9174)  time: 0.6297  data: 0.0406  max mem: 9465
Epoch: [7]  [ 400/1251]  eta: 0:09:27  lr: 0.000999  loss: 5.8446 (5.9084)  time: 0.6474  data: 0.0168  max mem: 9465
Epoch: [7]  [ 500/1251]  eta: 0:08:17  lr: 0.000999  loss: 5.8973 (5.8998)  time: 0.6515  data: 0.0086  max mem: 9465
Epoch: [7]  [ 600/1251]  eta: 0:07:09  lr: 0.000999  loss: 5.8266 (5.8858)  time: 0.6679  data: 0.0489  max mem: 9465
Epoch: [7]  [ 700/1251]  eta: 0:06:01  lr: 0.000999  loss: 5.8287 (5.8882)  time: 0.6159  data: 0.0207  max mem: 9465
Epoch: [7]  [ 800/1251]  eta: 0:04:55  lr: 0.000999  loss: 5.6807 (5.8801)  time: 0.6292  data: 0.0383  max mem: 9465
Epoch: [7]  [ 900/1251]  eta: 0:03:49  lr: 0.000999  loss: 5.7359 (5.8688)  time: 0.6148  data: 0.0141  max mem: 9465
Epoch: [7]  [1000/1251]  eta: 0:02:43  lr: 0.000999  loss: 5.8325 (5.8713)  time: 0.6205  data: 0.0081  max mem: 9465
Epoch: [7]  [1100/1251]  eta: 0:01:38  lr: 0.000999  loss: 5.7084 (5.8596)  time: 0.6626  data: 0.0516  max mem: 9465
Epoch: [7]  [1200/1251]  eta: 0:00:33  lr: 0.000999  loss: 5.7521 (5.8553)  time: 0.6613  data: 0.0533  max mem: 9465
Epoch: [7]  [1250/1251]  eta: 0:00:00  lr: 0.000999  loss: 5.7741 (5.8513)  time: 0.5430  data: 0.0058  max mem: 9465
Epoch: [7] Total time: 0:13:33 (0.6503 s / it)
Averaged stats: lr: 0.000999  loss: 5.7741 (5.8506)
Test:  [  0/163]  eta: 0:24:37  loss: 3.1098 (3.1098)  acc1: 35.5049 (35.5049)  acc5: 66.7752 (66.7752)  time: 9.0651  data: 8.8980  max mem: 9465
Test:  [ 10/163]  eta: 0:03:48  loss: 3.8994 (3.7043)  acc1: 23.7785 (26.5916)  acc5: 47.5570 (52.4430)  time: 1.4921  data: 1.3362  max mem: 9465
Test:  [ 20/163]  eta: 0:02:40  loss: 3.4557 (3.5609)  acc1: 30.2932 (30.3552)  acc5: 55.7003 (55.5142)  time: 0.7259  data: 0.5663  max mem: 9465
Test:  [ 30/163]  eta: 0:02:11  loss: 3.3664 (3.5363)  acc1: 32.5733 (29.9359)  acc5: 57.9805 (54.8807)  time: 0.7120  data: 0.5522  max mem: 9465
Test:  [ 40/163]  eta: 0:01:53  loss: 3.8279 (3.5820)  acc1: 20.5212 (27.7032)  acc5: 48.2085 (54.1591)  time: 0.7072  data: 0.5507  max mem: 9465
Test:  [ 50/163]  eta: 0:01:40  loss: 3.6738 (3.5677)  acc1: 20.5212 (27.6426)  acc5: 52.1173 (54.4038)  time: 0.7349  data: 0.5798  max mem: 9465
Test:  [ 60/163]  eta: 0:01:29  loss: 3.5991 (3.5632)  acc1: 28.6645 (28.1732)  acc5: 53.7459 (54.5095)  time: 0.7630  data: 0.6099  max mem: 9465
Test:  [ 70/163]  eta: 0:01:18  loss: 3.6017 (3.5976)  acc1: 25.7329 (27.9855)  acc5: 52.4430 (53.9203)  time: 0.7214  data: 0.5681  max mem: 9465
Test:  [ 80/163]  eta: 0:01:08  loss: 4.0518 (3.6750)  acc1: 25.7329 (26.9594)  acc5: 45.6026 (52.5596)  time: 0.6694  data: 0.5139  max mem: 9465
Test:  [ 90/163]  eta: 0:01:00  loss: 4.1082 (3.7298)  acc1: 18.5668 (26.3092)  acc5: 42.9967 (51.5696)  time: 0.7774  data: 0.6166  max mem: 9465
Test:  [100/163]  eta: 0:00:51  loss: 4.1082 (3.7779)  acc1: 20.8469 (25.7716)  acc5: 45.6026 (50.6724)  time: 0.8018  data: 0.6407  max mem: 9465
Test:  [110/163]  eta: 0:00:42  loss: 4.0577 (3.8069)  acc1: 22.4756 (25.3573)  acc5: 45.6026 (50.1159)  time: 0.6897  data: 0.5313  max mem: 9465
Test:  [120/163]  eta: 0:00:34  loss: 4.2480 (3.8448)  acc1: 20.8469 (24.9172)  acc5: 45.2769 (49.4468)  time: 0.6780  data: 0.5228  max mem: 9465
Test:  [130/163]  eta: 0:00:25  loss: 4.3250 (3.8807)  acc1: 18.2410 (24.5046)  acc5: 39.4137 (48.5988)  time: 0.6787  data: 0.5257  max mem: 9465
Test:  [140/163]  eta: 0:00:17  loss: 4.3973 (3.9182)  acc1: 16.6124 (23.9009)  acc5: 34.2020 (47.7233)  time: 0.6840  data: 0.5278  max mem: 9465
Test:  [150/163]  eta: 0:00:09  loss: 4.2305 (3.9260)  acc1: 19.5440 (23.8626)  acc5: 38.7622 (47.4729)  time: 0.6660  data: 0.5070  max mem: 9465
Test:  [160/163]  eta: 0:00:02  loss: 3.5973 (3.8935)  acc1: 26.3844 (24.4158)  acc5: 52.7687 (48.1498)  time: 0.5604  data: 0.4031  max mem: 9465
Test:  [162/163]  eta: 0:00:00  loss: 3.5973 (3.8827)  acc1: 29.3160 (24.6660)  acc5: 53.7594 (48.3480)  time: 0.5245  data: 0.3682  max mem: 9465
Test: Total time: 0:02:03 (0.7551 s / it)
* Acc@1 24.666 Acc@5 48.348 loss 3.883
Accuracy of the network on the 50000 test images: 24.7%
Max accuracy: 24.67%
Epoch: [8]  [   0/1251]  eta: 3:17:32  lr: 0.000999  loss: 6.0608 (6.0608)  time: 9.4744  data: 7.1942  max mem: 9465
Epoch: [8]  [ 100/1251]  eta: 0:14:28  lr: 0.000999  loss: 5.8727 (5.8445)  time: 0.6682  data: 0.0004  max mem: 9465
Epoch: [8]  [ 200/1251]  eta: 0:12:31  lr: 0.000999  loss: 6.0432 (5.8170)  time: 0.6743  data: 0.0009  max mem: 9465
Epoch: [8]  [ 300/1251]  eta: 0:11:06  lr: 0.000999  loss: 5.8264 (5.8059)  time: 0.6771  data: 0.0004  max mem: 9465
Epoch: [8]  [ 400/1251]  eta: 0:09:49  lr: 0.000999  loss: 5.7260 (5.7756)  time: 0.6591  data: 0.0009  max mem: 9465
Epoch: [8]  [ 500/1251]  eta: 0:08:36  lr: 0.000999  loss: 5.7935 (5.7603)  time: 0.6588  data: 0.0010  max mem: 9465
Epoch: [8]  [ 600/1251]  eta: 0:07:25  lr: 0.000999  loss: 5.6634 (5.7389)  time: 0.6632  data: 0.0004  max mem: 9465
Epoch: [8]  [ 700/1251]  eta: 0:06:16  lr: 0.000999  loss: 5.7733 (5.7363)  time: 0.6709  data: 0.0020  max mem: 9465
Epoch: [8]  [ 800/1251]  eta: 0:05:07  lr: 0.000999  loss: 5.6018 (5.7249)  time: 0.6279  data: 0.0005  max mem: 9465
Epoch: [8]  [ 900/1251]  eta: 0:03:58  lr: 0.000999  loss: 5.8211 (5.7233)  time: 0.6845  data: 0.0010  max mem: 9465
Epoch: [8]  [1000/1251]  eta: 0:02:50  lr: 0.000999  loss: 5.9391 (5.7191)  time: 0.6304  data: 0.0004  max mem: 9465
Epoch: [8]  [1100/1251]  eta: 0:01:42  lr: 0.000999  loss: 5.7756 (5.7153)  time: 0.6804  data: 0.0004  max mem: 9465
Epoch: [8]  [1200/1251]  eta: 0:00:34  lr: 0.000999  loss: 5.8585 (5.7098)  time: 0.6433  data: 0.0011  max mem: 9465
Epoch: [8]  [1250/1251]  eta: 0:00:00  lr: 0.000999  loss: 5.9486 (5.7075)  time: 0.5415  data: 0.0011  max mem: 9465
Epoch: [8] Total time: 0:14:04 (0.6748 s / it)
Averaged stats: lr: 0.000999  loss: 5.9486 (5.7037)
Test:  [  0/163]  eta: 0:25:39  loss: 2.7637 (2.7637)  acc1: 38.1107 (38.1107)  acc5: 74.5928 (74.5928)  time: 9.4428  data: 9.2854  max mem: 9465
Test:  [ 10/163]  eta: 0:04:01  loss: 3.5362 (3.4239)  acc1: 32.2476 (31.5961)  acc5: 56.0261 (57.3586)  time: 1.5777  data: 1.4237  max mem: 9465
Test:  [ 20/163]  eta: 0:02:51  loss: 3.0758 (3.2425)  acc1: 37.1335 (35.4428)  acc5: 60.2606 (60.2761)  time: 0.7898  data: 0.6367  max mem: 9465
Test:  [ 30/163]  eta: 0:02:21  loss: 3.0407 (3.2263)  acc1: 37.1335 (34.0443)  acc5: 60.2606 (59.9454)  time: 0.7820  data: 0.6294  max mem: 9465
Test:  [ 40/163]  eta: 0:02:01  loss: 3.3628 (3.2560)  acc1: 26.0586 (32.3349)  acc5: 55.3746 (59.4661)  time: 0.7635  data: 0.6107  max mem: 9465
Test:  [ 50/163]  eta: 0:01:47  loss: 3.2930 (3.2586)  acc1: 26.7101 (31.7621)  acc5: 57.3290 (59.4367)  time: 0.7811  data: 0.6280  max mem: 9465
Test:  [ 60/163]  eta: 0:01:35  loss: 3.2956 (3.2559)  acc1: 30.9446 (32.0179)  acc5: 57.3290 (59.3635)  time: 0.8116  data: 0.6562  max mem: 9465
Test:  [ 70/163]  eta: 0:01:23  loss: 3.2243 (3.2681)  acc1: 30.9446 (32.0962)  acc5: 57.3290 (59.2054)  time: 0.7697  data: 0.6096  max mem: 9465
Test:  [ 80/163]  eta: 0:01:13  loss: 3.6448 (3.3518)  acc1: 26.0586 (31.0009)  acc5: 49.8371 (57.6628)  time: 0.7287  data: 0.5687  max mem: 9465
Test:  [ 90/163]  eta: 0:01:04  loss: 3.8866 (3.4094)  acc1: 23.4528 (30.3469)  acc5: 48.5342 (56.5952)  time: 0.8241  data: 0.6686  max mem: 9465
Test:  [100/163]  eta: 0:00:54  loss: 3.8480 (3.4619)  acc1: 24.4300 (29.8223)  acc5: 49.1857 (55.6165)  time: 0.8334  data: 0.6761  max mem: 9465
Test:  [110/163]  eta: 0:00:45  loss: 3.7527 (3.4995)  acc1: 26.3844 (29.2279)  acc5: 49.5114 (54.9696)  time: 0.7343  data: 0.5758  max mem: 9465
Test:  [120/163]  eta: 0:00:36  loss: 3.8228 (3.5368)  acc1: 23.7785 (28.8072)  acc5: 47.8827 (54.2789)  time: 0.7137  data: 0.5572  max mem: 9465
Test:  [130/163]  eta: 0:00:27  loss: 3.9574 (3.5707)  acc1: 21.8241 (28.4134)  acc5: 46.5798 (53.4973)  time: 0.7008  data: 0.5457  max mem: 9465
Test:  [140/163]  eta: 0:00:19  loss: 4.0477 (3.6058)  acc1: 18.5668 (27.6988)  acc5: 42.3453 (52.7988)  time: 0.7160  data: 0.5606  max mem: 9465
Test:  [150/163]  eta: 0:00:10  loss: 3.8540 (3.6165)  acc1: 22.4756 (27.4975)  acc5: 46.5798 (52.5444)  time: 0.7152  data: 0.5594  max mem: 9465
Test:  [160/163]  eta: 0:00:02  loss: 3.2897 (3.5801)  acc1: 27.3616 (28.1547)  acc5: 56.6775 (53.2745)  time: 0.5720  data: 0.4189  max mem: 9465
Test:  [162/163]  eta: 0:00:00  loss: 3.2433 (3.5701)  acc1: 28.9902 (28.4140)  acc5: 56.6775 (53.4340)  time: 0.4947  data: 0.3428  max mem: 9465
Test: Total time: 0:02:09 (0.7952 s / it)
* Acc@1 28.414 Acc@5 53.434 loss 3.570
Accuracy of the network on the 50000 test images: 28.4%
Max accuracy: 28.41%
Epoch: [9]  [   0/1251]  eta: 3:12:04  lr: 0.000998  loss: 5.1575 (5.1575)  time: 9.2120  data: 7.5490  max mem: 9465
Epoch: [9]  [ 100/1251]  eta: 0:13:55  lr: 0.000998  loss: 6.0082 (5.6910)  time: 0.6409  data: 0.0011  max mem: 9465
Epoch: [9]  [ 200/1251]  eta: 0:11:58  lr: 0.000998  loss: 5.7437 (5.6709)  time: 0.6007  data: 0.0004  max mem: 9465
Epoch: [9]  [ 300/1251]  eta: 0:10:32  lr: 0.000998  loss: 5.5045 (5.6870)  time: 0.6052  data: 0.0004  max mem: 9465
Epoch: [9]  [ 400/1251]  eta: 0:09:20  lr: 0.000998  loss: 5.8869 (5.6536)  time: 0.6273  data: 0.0004  max mem: 9465
Epoch: [9]  [ 500/1251]  eta: 0:08:09  lr: 0.000998  loss: 5.5326 (5.6307)  time: 0.6366  data: 0.0004  max mem: 9465
Epoch: [9]  [ 600/1251]  eta: 0:07:02  lr: 0.000998  loss: 5.7352 (5.6255)  time: 0.6192  data: 0.0004  max mem: 9465
Epoch: [9]  [ 700/1251]  eta: 0:05:55  lr: 0.000998  loss: 5.7762 (5.6182)  time: 0.6045  data: 0.0004  max mem: 9465
Epoch: [9]  [ 800/1251]  eta: 0:04:50  lr: 0.000998  loss: 5.7267 (5.6187)  time: 0.6157  data: 0.0004  max mem: 9465
Epoch: [9]  [ 900/1251]  eta: 0:03:45  lr: 0.000998  loss: 5.6810 (5.6035)  time: 0.6505  data: 0.0004  max mem: 9465
Epoch: [9]  [1000/1251]  eta: 0:02:41  lr: 0.000998  loss: 5.5511 (5.5918)  time: 0.6516  data: 0.0004  max mem: 9465
Epoch: [9]  [1100/1251]  eta: 0:01:36  lr: 0.000998  loss: 5.6897 (5.5880)  time: 0.6335  data: 0.0004  max mem: 9465
Epoch: [9]  [1200/1251]  eta: 0:00:32  lr: 0.000998  loss: 5.5622 (5.5736)  time: 0.6515  data: 0.0004  max mem: 9465
Epoch: [9]  [1250/1251]  eta: 0:00:00  lr: 0.000998  loss: 5.6794 (5.5718)  time: 0.5484  data: 0.0013  max mem: 9465
Epoch: [9] Total time: 0:13:18 (0.6386 s / it)
Averaged stats: lr: 0.000998  loss: 5.6794 (5.5953)
Test:  [  0/163]  eta: 0:25:59  loss: 2.6523 (2.6523)  acc1: 43.3225 (43.3225)  acc5: 75.2443 (75.2443)  time: 9.5688  data: 9.4111  max mem: 9465
Test:  [ 10/163]  eta: 0:04:02  loss: 3.0938 (3.0020)  acc1: 36.4821 (38.3180)  acc5: 60.9120 (64.4655)  time: 1.5872  data: 1.4295  max mem: 9465
Test:  [ 20/163]  eta: 0:02:52  loss: 2.8008 (2.9073)  acc1: 39.4137 (40.9338)  acc5: 65.7980 (65.8136)  time: 0.7905  data: 0.6294  max mem: 9465
Test:  [ 30/163]  eta: 0:02:20  loss: 2.8484 (2.8854)  acc1: 39.4137 (39.9285)  acc5: 65.7980 (66.1028)  time: 0.7661  data: 0.6049  max mem: 9465
Test:  [ 40/163]  eta: 0:02:00  loss: 3.0945 (2.9350)  acc1: 28.9902 (37.1097)  acc5: 62.8664 (65.5279)  time: 0.7446  data: 0.5863  max mem: 9465
Test:  [ 50/163]  eta: 0:01:47  loss: 3.0639 (2.9331)  acc1: 29.3160 (37.0122)  acc5: 64.1694 (65.5681)  time: 0.7840  data: 0.6271  max mem: 9465
Test:  [ 60/163]  eta: 0:01:35  loss: 2.9661 (2.9257)  acc1: 37.1335 (37.4272)  acc5: 66.1238 (65.7340)  time: 0.8128  data: 0.6542  max mem: 9465
Test:  [ 70/163]  eta: 0:01:23  loss: 3.0433 (2.9689)  acc1: 36.1563 (36.9959)  acc5: 64.4951 (64.8805)  time: 0.7497  data: 0.5875  max mem: 9465
Test:  [ 80/163]  eta: 0:01:12  loss: 3.4973 (3.0540)  acc1: 29.6417 (35.8025)  acc5: 53.7459 (63.3088)  time: 0.6978  data: 0.5401  max mem: 9465
Test:  [ 90/163]  eta: 0:01:03  loss: 3.6141 (3.1195)  acc1: 28.6645 (34.9393)  acc5: 51.7915 (62.1183)  time: 0.8018  data: 0.6482  max mem: 9465
Test:  [100/163]  eta: 0:00:53  loss: 3.5025 (3.1614)  acc1: 29.6417 (34.6309)  acc5: 54.0717 (61.4442)  time: 0.7909  data: 0.6364  max mem: 9465
Test:  [110/163]  eta: 0:00:44  loss: 3.4967 (3.2062)  acc1: 25.7329 (33.8234)  acc5: 55.0489 (60.6509)  time: 0.6686  data: 0.5117  max mem: 9465
Test:  [120/163]  eta: 0:00:35  loss: 3.7668 (3.2490)  acc1: 26.3844 (33.3540)  acc5: 49.8371 (59.8029)  time: 0.6848  data: 0.5287  max mem: 9465
Test:  [130/163]  eta: 0:00:26  loss: 3.8402 (3.2860)  acc1: 26.3844 (32.9189)  acc5: 49.1857 (58.9651)  time: 0.7042  data: 0.5503  max mem: 9465
Test:  [140/163]  eta: 0:00:18  loss: 3.8676 (3.3309)  acc1: 23.4528 (32.0396)  acc5: 46.9055 (58.1121)  time: 0.6682  data: 0.5120  max mem: 9465
Test:  [150/163]  eta: 0:00:10  loss: 3.6613 (3.3392)  acc1: 23.4528 (31.8938)  acc5: 50.8143 (57.9416)  time: 0.6536  data: 0.4977  max mem: 9465
Test:  [160/163]  eta: 0:00:02  loss: 3.0126 (3.3002)  acc1: 37.4593 (32.6724)  acc5: 63.8436 (58.7068)  time: 0.6104  data: 0.4571  max mem: 9465
Test:  [162/163]  eta: 0:00:00  loss: 3.0126 (3.2890)  acc1: 37.7850 (32.9400)  acc5: 63.8436 (58.8680)  time: 0.5440  data: 0.3919  max mem: 9465
Test: Total time: 0:02:08 (0.7880 s / it)
* Acc@1 32.940 Acc@5 58.868 loss 3.289
Accuracy of the network on the 50000 test images: 32.9%
Max accuracy: 32.94%
Epoch: [10]  [   0/1251]  eta: 2:41:07  lr: 0.000998  loss: 6.0824 (6.0824)  time: 7.7279  data: 7.1785  max mem: 9465
Epoch: [10]  [ 100/1251]  eta: 0:13:35  lr: 0.000998  loss: 5.2946 (5.5510)  time: 0.6604  data: 0.0592  max mem: 9465
